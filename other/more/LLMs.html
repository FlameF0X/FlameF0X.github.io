<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>GPT Forward Pass - Math Explanation</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 2rem;
      background-color: #f7f7f7;
    }
    h1, h2, h3 {
      color: #222;
    }
    .math-block {
      background: #fff;
      padding: 1rem;
      margin: 1rem 0;
      border-left: 4px solid #0077cc;
      overflow-x: auto;
    }
    code {
      background: #eee;
      padding: 2px 4px;
      border-radius: 3px;
    }
    pre {
      background: #eee;
      padding: 1rem;
      overflow-x: auto;
    }
  </style>
</head>
<body>
  <h1>Understanding GPT: Mathematical Foundations</h1>

  <h2>1. Tokenization & Embedding Lookup</h2>
  <p>Each token is mapped to an index, then embedded into a continuous vector space:</p>
  <div class="math-block">
    X = \begin{bmatrix}
    \text{Embed}("the") \\
    \text{Embed}("cat") \\
    \text{Embed}("sat")
    \end{bmatrix} \in \mathbb{R}^{3 \times 4}
  </div>

  <h2>2. Linear Projections</h2>
  <p>Compute queries, keys, and values:</p>
  <div class="math-block">
    Q = X W^Q, \quad K = X W^K, \quad V = X W^V
  </div>

  <h2>3. Scaled Dot-Product Attention</h2>
  <p>Compare tokens via dot product, scaled by the dimension:</p>
  <div class="math-block">
    \text{Scores} = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}
  </div>

  <h2>4. Attention Weights (Softmax)</h2>
  <div class="math-block">
    \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
  </div>
  <p>This ensures each row of attention weights sums to 1.</p>

  <h2>5. Weighted Sum for Context</h2>
  <div class="math-block">
    \text{AttentionOutput} = \text{AttentionWeights} \cdot V
  </div>

  <h2>6. Feed-Forward Network</h2>
  <p>Each token vector goes through a nonlinear MLP:</p>
  <div class="math-block">
    h_1 = \text{ReLU}(zW_1 + b_1), \quad h_2 = h_1W_2 + b_2
  </div>

  <h2>7. Final Representation</h2>
  <p>Extract the final token's output to predict the next word:</p>
  <div class="math-block">
    h_{\text{final}} = \text{FFNOutput}[-1]
  </div>

  <h2>8. Output Logits</h2>
  <div class="math-block">
    \text{Logits} = h_{\text{final}} W_{\text{out}}^T + b_{\text{out}} \in \mathbb{R}^{|\mathcal{V}|}
  </div>

  <h2>9. Softmax for Probabilities</h2>
  <div class="math-block">
    P(y_i) = \frac{e^{\text{Logits}_i}}{\sum_j e^{\text{Logits}_j}}
  </div>
  <p>This gives a valid probability distribution over the vocabulary.</p>

  <h2>ðŸ“Œ Not Just Matrix Math</h2>
  <ul>
    <li><strong>Softmax:</strong> Makes scores probabilistic.</li>
    <li><strong>ReLU/GELU:</strong> Adds non-linearity.</li>
    <li><strong>Attention:</strong> Dynamic token interaction.</li>
    <li><strong>Probability:</strong> Output is a distribution.</li>
    <li><strong>Training:</strong> Backpropagation, loss gradients.</li>
    <li><strong>Positional encoding (not shown):</strong> Adds order to input.</li>
  </ul>

  <h2>Want to go further?</h2>
  <p>Try extending this with math.js or Pyodide to compute values directly in-browser.</p>

</body>
</html>
