<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Low-Level Mathematical Exposition of Generative Transformers</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            background-color: #121212;
            color: #E0E0E0;
            font-family: 'Inter', sans-serif;
            line-height: 1.6;
            overflow-x: hidden;
        }
        .paper-container {
            max-width: 900px;
            margin: auto;
            padding: 2rem;
        }
        .header {
            text-align: center;
            margin-bottom: 2rem;
        }
        .header h1 {
            font-size: 2.5rem;
            font-weight: 700;
        }
        .header p {
            font-size: 1rem;
            color: #B0B0B0;
        }
        .abstract {
            background-color: #1A1A1A;
            border-left: 4px solid #3B82F6;
            padding: 1.5rem;
            margin-bottom: 2rem;
            border-radius: 0.5rem;
        }
        .section-title {
            font-size: 1.8rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: #F5F5F5;
        }
        .subsection-title {
            font-size: 1.4rem;
            font-weight: 500;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
            color: #C0C0C0;
        }
        pre {
            background-color: #1E1E1E;
            color: #D4D4D4;
            padding: 1rem;
            border-radius: 0.5rem;
            font-family: 'Fira Code', monospace;
            display: block;
            width: 100%;
            box-sizing: border-box;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 1rem;
            margin-bottom: 1rem;
        }
        th, td {
            border: 1px solid #333;
            padding: 0.5rem;
            text-align: left;
        }
        th {
            background-color: #2D2D2D;
            color: #E0E0E0;
        }
        .paper-container p, .paper-container li {
            text-align: justify;
        }
    </style>
</head>
<body>

<div class="paper-container">
    <header class="header">
        <h1 class="text-3xl font-bold">A Low-Level Mathematical Exposition of Generative Transformers</h1>
        <p class="mt-2 text-sm">Daniel Fox and Gemini AI</p>
    </header>

    <div class="abstract">
        <h2 class="text-xl font-semibold mb-2">Abstract</h2>
        <p>This paper provides an in-depth, from-scratch explanation of how a Generative Pre-trained Transformer (GPT) model predicts the next token. It moves beyond computational abstractions to explore the core mathematical operations that form the basis of the model. We detail how matrices and vectors are manipulated and then apply these primitives to construct a complete forward and backward pass for a single transformer layer. Using the example phrase "The cat sat on the", this document demonstrates the direct relationship between advanced linear algebra and calculus in a working computational model.</p>
    </div>

    <main>
        <section>
            <h2 class="section-title">1. Core Computational Primitives</h2>
            <p>At its heart, a neural network is a series of matrix and vector operations. Before we build the transformer, we must first define the low-level functions that perform these fundamental calculations. We represent matrices as lists of lists and vectors as simple lists.</p>

            <div class="mt-4">
                <div class="subsection-title">1.1. Matrix Multiplication</div>
                <p>The product of two matrices, $C = A \cdot B$, is defined such that the element at row $i$ and column $j$ of $C$ is the sum of the products of the elements in row $i$ of $A$ and column $j$ of $B$.</p>
                $$ C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj} $$
            </div>

            <div class="mt-4">
                <div class="subsection-title">1.2. Matrix Transposition</div>
                <p>Transposition involves flipping a matrix over its diagonal, converting the rows into columns and the columns into rows.</p>
                $$ B_{ij} = A_{ji} $$
            </div>

            <div class="mt-4">
                <div class="subsection-title">1.3. Element-wise Operations (Addition, etc.)</div>
                <p>These operations are applied to each corresponding element of two matrices of the same size, or between a matrix and a single scalar.</p>
                $$ C_{ij} = A_{ij} + B_{ij} $$
            </div>
        </section>

        <section>
            <h2 class="section-title">2. The Forward Pass: From Embeddings to Prediction</h2>
            <p>The forward pass takes an input sequence and processes it through the model's layers to produce a final prediction.</p>

            <div class="mt-4">
                <div class="subsection-title">2.1. Tokenization and Embedding</div>
                <p>The input sequence "The cat sat on the" is tokenized and then mapped to embedding vectors, which are the model's internal representation of each word. The input is a matrix $X$ of shape $[sequence\_length \times d_{model}]$.</p>
            </div>
            
            <div class="mt-4">
                <div class="subsection-title">2.2. Self-Attention Mechanism (Full Math)</div>
                <p>This is the core of the transformer. The input matrix $X$ is transformed into three matrices: Query ($Q$), Key ($K$), and Value ($V$). The attention scores are calculated as the dot product of $Q$ and $K$ and then scaled. A softmax function turns these scores into probabilities, which are then multiplied by the $V$ matrix to get the final attention output.</p>
                <p>Mathematical Formula:</p>
                $$ \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
                <p>Where:</p>
                <ul>
                    <li>$Q = X \cdot W_Q$</li>
                    <li>$K = X \cdot W_K$</li>
                    <li>$V = X \cdot W_V$</li>
                    <li>$W_Q$, $W_K$, $W_V$ are the weight matrices for the Query, Key, and Value vectors.</li>
                </ul>
                <p>The scores are calculated as:</p>
                $$ \text{Scores} = Q K^T $$
                <p>The scaled scores are:</p>
                $$ \text{Scores}_{scaled} = \frac{\text{Scores}}{\sqrt{d_k}} $$
                <p>A causal mask is applied to the scaled scores to prevent a token from attending to future tokens. The masked scores are then passed through the softmax function to get the attention probabilities:</p>
                $$ \text{Softmax}(\mathbf{z}_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}} $$
                <p>The final attention output, $Z_{attn}$, is a matrix of shape $[sequence\_length \times d_k]$:</p>
                $$ Z_{attn} = \text{Softmax}(\text{Scores}_{scaled}) \cdot V $$
                <p>This output is then passed through an output weight matrix $W_O$ and added to the original input $X$ (the residual connection), before being normalized.</p>
            </div>
            
            <div class="mt-4">
                <div class="subsection-title">2.3. Feed-Forward Network</div>
                <p>The output of the attention block, $Z$, is passed through a simple feed-forward network. This network consists of two linear layers with a non-linear activation function (ReLU) in between.</p>
                $$ \text{FFN}(\mathbf{x}) = \text{ReLU}(\mathbf{x}W_1 + \mathbf{b}_1)W_2 + \mathbf{b}_2 $$
            </div>

            <div class="mt-4">
                <div class="subsection-title">2.4. Final Linear Layer to Logits</div>
                <p>The output of the FFN, $\text{Output}_{final}$, is passed through the final linear layer to produce logits for each token in the vocabulary.</p>
                $$ \mathbf{l}_{final} = \text{Output}_{final} \cdot W_{final} + \mathbf{b}_{final} $$
                <p>The final logits vector $\mathbf{l}_{final}$ is what the model uses to make its prediction.</p>
            </div>
        </section>

        <section>
            <h2 class="section-title">3. The Backward Pass: The Iterative Training Loop (Full Math)</h2>
            <p>The backward pass is the process of computing the gradient of the loss with respect to each model parameter. We start with the loss, and use the chain rule to backpropagate the gradients through the network, layer by layer.</p>

            <div class="mt-4">
                <div class="subsection-title">3.1. Loss and Initial Gradient Calculation</div>
                <p>We use Cross-Entropy Loss to measure the error. The gradient of the loss with respect to the output logits is simply the difference between the predicted probabilities and the one-hot encoded true label.</p>
                <p>Initial Gradient:</p>
                $$ \frac{\partial L}{\partial \mathbf{l}} = \hat{y} - y_{true} $$
            </div>
            
            <div class="mt-4">
                <div class="subsection-title">3.2. Backpropagating through the Final Linear Layer</div>
                <p>We compute the gradients for $W_{final}$ and $b_{final}$ using the chain rule.</p>
                $$ \frac{\partial L}{\partial W_{final}} = \text{Output}_{final}^T \cdot \frac{\partial L}{\partial \mathbf{l}} $$
                $$ \frac{\partial L}{\partial \mathbf{b}_{final}} = \frac{\partial L}{\partial \mathbf{l}} $$
                <p>The gradient passed back to the FFN is:</p>
                $$ \frac{\partial L}{\partial \text{Output}_{final}} = \frac{\partial L}{\partial \mathbf{l}} \cdot W_{final}^T $$
            </div>

            <div class="mt-4">
                <div class="subsection-title">3.3. Backpropagating through the Feed-Forward Network</div>
                <p>The FFN has a linear layer, a ReLU activation, and another linear layer. We apply the chain rule to each component.</p>
                $$ \frac{\partial L}{\partial W_{ffn2}} = \text{Output}_{ffn1}^T \cdot \frac{\partial L}{\partial \text{Output}_{final}} $$
                $$ \frac{\partial L}{\partial \mathbf{b}_{ffn2}} = \sum_{i=1}^{sequence\_length} \frac{\partial L}{\partial \text{Output}_{final}^{(i)}} $$
                <p>The gradient through the ReLU activation is:</p>
                $$ \frac{\partial L}{\partial \text{Output}_{ffn1}} = \left( \frac{\partial L}{\partial \text{Output}_{final}} \cdot W_{ffn2}^T \right) \odot \text{ReLU}'(\text{FFN1}_{\text{output}}) $$
                <p>where $\odot$ denotes element-wise multiplication.</p>
                <p>The gradients for the first FFN layer are:</p>
                $$ \frac{\partial L}{\partial W_{ffn1}} = Z^T \cdot \frac{\partial L}{\partial \text{Output}_{ffn1}} $$
                $$ \frac{\partial L}{\partial \mathbf{b}_{ffn1}} = \sum_{i=1}^{sequence\_length} \frac{\partial L}{\partial \text{Output}_{ffn1}^{(i)}} $$
            </div>

            <div class="mt-4">
                <div class="subsection-title">3.4. Backpropagating through the Self-Attention Mechanism (Full Derivation)</div>
                <p>This is the most complex part. We must find the gradients for $W_Q, W_K, W_V$ by backpropagating through the entire attention formula. Let's start from the gradient of the attention output $Z_{attn}$ and work backward.</p>
                $$ \frac{\partial L}{\partial Z_{attn}} = \frac{\partial L}{\partial Z} \cdot W_O^T $$
                <p>The derivative of the final attention output with respect to the attention probabilities and the Value matrix are:</p>
                $$ \frac{\partial L}{\partial \text{Softmax}(...) } = \frac{\partial L}{\partial Z_{attn}} \cdot V^T $$
                $$ \frac{\partial L}{\partial V } = \text{Softmax}(...)^T \cdot \frac{\partial L}{\partial Z_{attn}} $$
                <p>The derivative of the softmax is complex, but we can simplify the calculation of the gradient with respect to the attention scores ($S$) as:</p>
                $$ \frac{\partial L}{\partial S_{ij}} = \sum_{k} \frac{\partial L}{\partial \text{Softmax}(S)_{ik}} \cdot \left( \text{Softmax}(S)_{ik} (\delta_{jk} - \text{Softmax}(S)_{jk}) \right) $$
                <p>Using this, we can derive the gradients for the weights:</p>
                $$ \frac{\partial L}{\partial W_v} = X^T \cdot \frac{\partial L}{\partial V} $$
                $$ \frac{\partial L}{\partial W_k} = X^T \cdot \frac{\partial L}{\partial K} $$
                $$ \frac{\partial L}{\partial W_q} = X^T \cdot \frac{\partial L}{\partial Q} $$
            </div>
            
            <div class="mt-4">
                <div class="subsection-title">3.5. Parameter Update (Gradient Descent)</div>
                <p>The final step is to update the weights and biases by subtracting the gradients multiplied by the learning rate. This adjusts the parameters in the direction that minimizes the loss.</p>
                $$ \text{Weight}_{new} = \text{Weight}_{old} - \alpha \cdot \frac{\partial L}{\partial \text{Weight}_{old}} $$
            </div>
        </section>

        <section>
            <h2 class="section-title">4. A Concrete Numerical Example: "The cat sat on the mat"</h2>
            <p>To ground the theoretical concepts in a tangible example, let's trace the forward and backward pass for the phrase "The cat sat on the mat". We will use simplified dimensions for clarity. Let's assume a sequence length of 5, a model dimension ($d_{model}$) of 8, and an attention head dimension ($d_k$) of 4. The target token is "mat".</p>
            
            <div class="mt-4">
                <div class="subsection-title">4.1. Forward Pass Example</div>
                <p>The input embeddings for "The cat sat on the" form a matrix $X$ of shape $[5 \times 8]$.</p>
                <p>The attention matrices are then calculated:</p>
                <ul>
                    <li>$Q = X_{5\times8} \cdot W_{Q_{8\times4}}$ results in a matrix of shape $[5 \times 4]$.</li>
                    <li>$K = X_{5\times8} \cdot W_{K_{8\times4}}$ results in a matrix of shape $[5 \times 4]$.</li>
                    <li>$V = X_{5\times8} \cdot W_{V_{8\times4}}$ results in a matrix of shape $[5 \times 4]$.</li>
                </ul>
                <p>The attention scores are computed as:</p>
                $$ \text{Scores} = Q_{5\times4} \cdot K^T_{4\times5} $$
                <p>This results in a matrix of shape $[5 \times 5]$, where each row represents how much a token attends to all previous tokens in the sequence. A causal mask is applied to zero out future tokens.</p>
                <p>The final attention output for this layer, $Z_{attn}$, will have a shape of $[5 \times 4]$.</p>
                <p>This output is then projected and added to the residual connection to form a new output $Z$ of shape $[5 \times 8]$.</p>
                <p>The FFN takes this $[5 \times 8]$ matrix and processes it, eventually producing a final output of shape $[5 \times 8]$. The final linear layer then projects this to the vocabulary size, for example, 1000 tokens.</p>
                <p>The final logits vector for the last token "the" will be of shape $[1 \times 1000]$.</p>
            </div>

            <div class="mt-4">
                <div class="subsection-title">4.2. Backward Pass Example</div>
                <p>The backward pass begins with the gradient of the loss with respect to the final logits, $\frac{\partial L}{\partial \mathbf{l}}$, which is a vector of shape $[1 \times 1000]$.</p>
                <p>The gradient for the final weight matrix $W_{final}$ is calculated from the outer product of the final FFN output (shape $[5 \times 8]$) and the logits gradient (shape $[1 \times 1000]$), resulting in a gradient matrix of shape $[8 \times 1000]$.</p>
                <p>This gradient is then backpropagated through the FFN, scaling appropriately to get gradients for $W_{ffn1}$ and $W_{ffn2}$ and their biases. The gradient passed to the attention block, $\frac{\partial L}{\partial Z}$, is a matrix of shape $[5 \times 8]$.</p>
                <p>This gradient is then used to compute the gradients for the attention weights $W_Q, W_K, W_V$. For instance, the gradient $\frac{\partial L}{\partial Q}$ is a matrix of shape $[5 \times 4]$, which is then used to compute the gradient for $W_Q$ by multiplying with the input matrix $X^T_{8\times5}$, resulting in a gradient matrix of shape $[8 \times 4]$.</p>
                <p>Finally, all these gradient matrices are used to update the respective weight matrices via gradient descent.</p>
            </div>
        </section>

        <section>
            <h2 class="section-title">5. Final Prediction: Sampling the Next Token</h2>
            <p>After the forward pass is complete, we must convert the final logits into a token prediction. The logits are passed through a softmax function to get a probability distribution, and the token with the highest probability is selected.</p>
            
            <div class="mt-4">
                <div class="subsection-title">Softmax on the final logits</div>
                $$ P(\text{token}_i) = \frac{e^{l_i}}{\sum_{j} e^{l_j}} $$
                <p>The predicted token is the one corresponding to the index with the maximum probability.</p>
            </div>
        </section>

        <section>
            <h2 class="section-title">Conclusion</h2>
            <p>This document has demonstrated that the seemingly magical ability of a GPT model to predict text is, in fact, a series of deterministic mathematical operations. By breaking down the components and tracing the chain of derivatives through the backward pass, we can see that a transformer is an elegant system of linear algebra and calculus.</p>
        </section>
    </main>
</div>

</body>
</html>
